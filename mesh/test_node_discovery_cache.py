#!/usr/bin/env python3
"""
Test Node Discovery Caching System
Tests the comprehensive caching functionality for node discovery.
"""

import asyncio
import logging
import time
import sys
import os

# Add the current directory to the Python path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from Mesh_Network.Node_Discovery_Cache import (
    NodeDiscoveryCache, 
    NodeCacheEntry, 
    CacheStrategy, 
    NodeStatus
)
from Mesh_Network.Advanced_Node_Discovery import (
    AdvancedNodeDiscovery,
    AdvancedMeshNode,
    GeoLocation,
    NetworkMetrics,
    NodeCapabilities,
    NodeType
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

class NodeDiscoveryCacheTest:
    """Comprehensive test suite for node discovery caching"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.cache = NodeDiscoveryCache(
            cache_dir="test_cache",
            max_cache_size=50,
            default_ttl=300,  # 5 minutes for testing
            strategy=CacheStrategy.HYBRID
        )
        self.discovery = AdvancedNodeDiscovery()
    
    async def test_basic_caching(self):
        """Test basic node caching functionality"""
        print("\nüîÑ Testing Basic Caching...")
        print("=" * 50)
        
        # Test adding nodes to cache
        nodes_to_cache = [
            {"id": "test_node_1", "host": "127.0.0.1", "port": 8001},
            {"id": "test_node_2", "host": "127.0.0.1", "port": 8002},
            {"id": "test_node_3", "host": "192.168.1.10", "port": 8003},
        ]
        
        for node_data in nodes_to_cache:
            entry = self.cache.cache_node(
                node_id=node_data["id"],
                host=node_data["host"],
                port=node_data["port"],
                location={
                    "latitude": 40.7128,
                    "longitude": -74.0060,
                    "country": "USA",
                    "city": "New York",
                    "region": "NY"
                },
                metrics={
                    "latency": 50.0 + (int(node_data["port"]) % 10),
                    "bandwidth": 100.0,
                    "uptime": 99.0
                },
                discovery_method="test",
                tags={"test", "active"}
            )
            print(f"‚úÖ Cached node: {node_data['id']} ({node_data['host']}:{node_data['port']})")
        
        # Test retrieving nodes from cache
        print("\nüîç Testing Node Retrieval...")
        for node_data in nodes_to_cache:
            cached_node = self.cache.get_node(node_data["id"])
            if cached_node:
                print(f"‚úÖ Retrieved: {cached_node.node_id} - Status: {cached_node.status.value}")
            else:
                print(f"‚ùå Failed to retrieve: {node_data['id']}")
        
        # Test host:port lookup
        print("\nüîç Testing Host:Port Lookup...")
        test_node = self.cache.get_node_by_host_port("127.0.0.1", 8001)
        if test_node:
            print(f"‚úÖ Found node by host:port: {test_node.node_id}")
        else:
            print("‚ùå Failed to find node by host:port")
        
        # Test cache statistics
        stats = self.cache.get_cache_stats()
        print(f"\nüìä Cache Statistics:")
        print(f"  Cache Size: {stats['cache_size']}")
        print(f"  Hit Rate: {stats['hit_rate']:.1f}%")
        print(f"  Active Nodes: {stats['active_nodes']}")
        
        return len(nodes_to_cache)
    
    async def test_metrics_update(self):
        """Test updating node metrics"""
        print("\nüîÑ Testing Metrics Update...")
        print("=" * 50)
        
        # Get a cached node
        test_node = self.cache.get_node("test_node_1")
        if not test_node:
            print("‚ùå No test node found for metrics update")
            return
        
        print(f"üìä Initial metrics for {test_node.node_id}:")
        print(f"  Probe count: {test_node.probe_count}")
        print(f"  Average latency: {test_node.average_latency:.1f}ms")
        print(f"  Reliability score: {test_node.reliability_score:.3f}")
        
        # Simulate multiple measurements
        latencies = [45.0, 52.0, 48.0, 55.0, 50.0]
        
        print(f"\nüîÑ Updating metrics with latencies: {latencies}")
        for latency in latencies:
            self.cache.update_node_metrics("test_node_1", latency, success=True)
        
        # Check updated metrics
        updated_node = self.cache.get_node("test_node_1")
        print(f"\nüìä Updated metrics for {updated_node.node_id}:")
        print(f"  Probe count: {updated_node.probe_count}")
        print(f"  Average latency: {updated_node.average_latency:.1f}ms")
        print(f"  Best latency: {updated_node.best_latency:.1f}ms")
        print(f"  Worst latency: {updated_node.worst_latency:.1f}ms")
        print(f"  Reliability score: {updated_node.reliability_score:.3f}")
        
        # Test failed connection
        print(f"\nüîÑ Testing failed connection...")
        self.cache.update_node_metrics("test_node_1", float('inf'), success=False)
        
        failed_node = self.cache.get_node("test_node_1")
        print(f"  Failed connections: {failed_node.failed_connections}")
        print(f"  Reliability score: {failed_node.reliability_score:.3f}")
    
    async def test_location_and_tag_queries(self):
        """Test location and tag-based queries"""
        print("\nüîÑ Testing Location and Tag Queries...")
        print("=" * 50)
        
        # Add nodes with different locations
        locations = [
            {"country": "USA", "region": "NY", "city": "New York"},
            {"country": "USA", "region": "CA", "city": "San Francisco"},
            {"country": "UK", "region": "England", "city": "London"},
        ]
        
        for i, location in enumerate(locations, 4):
            self.cache.cache_node(
                node_id=f"geo_node_{i}",
                host="127.0.0.1",
                port=8000 + i,
                location=location,
                tags={"geo_test", f"region_{location['region']}"}
            )
        
        # Test location queries
        print(f"\nüåç Testing location queries...")
        usa_nodes = self.cache.get_nodes_by_location(country="USA")
        print(f"  USA nodes: {len(usa_nodes)}")
        
        ny_nodes = self.cache.get_nodes_by_location(country="USA", region="NY")
        print(f"  NY nodes: {len(ny_nodes)}")
        
        # Test tag queries
        print(f"\nüè∑Ô∏è  Testing tag queries...")
        geo_test_nodes = self.cache.get_nodes_by_tag("geo_test")
        print(f"  Geo test nodes: {len(geo_test_nodes)}")
        
        region_ca_nodes = self.cache.get_nodes_by_tag("region_CA")
        print(f"  CA region nodes: {len(region_ca_nodes)}")
    
    async def test_cache_persistence(self):
        """Test cache persistence functionality"""
        print("\nüîÑ Testing Cache Persistence...")
        print("=" * 50)
        
        # Save current cache
        print("üíæ Saving cache...")
        self.cache.save_cache()
        
        # Get current cache size
        original_size = len(self.cache.cache)
        print(f"  Original cache size: {original_size}")
        
        # Clear cache
        print("üßπ Clearing in-memory cache...")
        self.cache.cache.clear()
        self.cache.host_port_index.clear()
        self.cache.location_index.clear()
        self.cache.tag_index.clear()
        
        print(f"  Cache size after clear: {len(self.cache.cache)}")
        
        # Reload cache
        print("üì• Reloading cache from disk...")
        self.cache._load_cache()
        
        reloaded_size = len(self.cache.cache)
        print(f"  Reloaded cache size: {reloaded_size}")
        
        if reloaded_size == original_size:
            print("‚úÖ Cache persistence test passed!")
        else:
            print("‚ùå Cache persistence test failed!")
        
        return reloaded_size == original_size
    
    async def test_cache_expiration(self):
        """Test cache expiration functionality"""
        print("\nüîÑ Testing Cache Expiration...")
        print("=" * 50)
        
        # Add a node with short TTL
        short_ttl_entry = self.cache.cache_node(
            node_id="short_lived_node",
            host="127.0.0.1",
            port=9999,
            ttl=1,  # 1 second TTL
            tags={"short_lived"}
        )
        
        print(f"‚úÖ Added short-lived node with 1 second TTL")
        
        # Verify node exists
        node = self.cache.get_node("short_lived_node")
        if node:
            print(f"‚úÖ Node found immediately after creation")
        
        # Wait for expiration
        print("‚è≥ Waiting 2 seconds for expiration...")
        await asyncio.sleep(2)
        
        # Try to retrieve expired node
        expired_node = self.cache.get_node("short_lived_node")
        if expired_node:
            print("‚ùå Expired node still found (should have been removed)")
        else:
            print("‚úÖ Expired node correctly removed from cache")
        
        # Test manual cleanup
        print("\nüßπ Testing manual cleanup...")
        before_cleanup = len(self.cache.cache)
        cleaned = self.cache.cleanup_expired()
        after_cleanup = len(self.cache.cache)
        
        print(f"  Nodes before cleanup: {before_cleanup}")
        print(f"  Nodes cleaned: {cleaned}")
        print(f"  Nodes after cleanup: {after_cleanup}")
    
    async def test_integrated_discovery(self):
        """Test integration with actual node discovery"""
        print("\nüîÑ Testing Integrated Discovery...")
        print("=" * 50)
        
        # Create a local node for testing
        local_node = AdvancedMeshNode(
            id="test_local_node",
            host="127.0.0.1",
            port=8000,
            location=GeoLocation(40.7128, -74.0060, "USA", "New York", "NY"),
            metrics=NetworkMetrics(latency=25.0, bandwidth=100.0, uptime=99.0),
            capabilities=NodeCapabilities(max_connections=100)
        )
        
        print(f"üåê Created local node: {local_node.id}")
        
        # Run discovery
        print("üîç Running node discovery...")
        discovered_nodes = await self.discovery.discover_nodes(local_node)
        
        print(f"‚úÖ Discovered {len(discovered_nodes)} nodes")
        
        # Cache discovered nodes
        print("üíæ Caching discovered nodes...")
        for node in discovered_nodes:
            location_dict = None
            if node.location:
                location_dict = {
                    'latitude': node.location.latitude,
                    'longitude': node.location.longitude,
                    'country': node.location.country,
                    'city': node.location.city,
                    'region': node.location.region
                }
            
            self.cache.cache_node(
                node_id=node.id,
                host=node.host,
                port=node.port,
                location=location_dict,
                metrics={
                    'latency': node.metrics.latency,
                    'bandwidth': node.metrics.bandwidth,
                    'uptime': node.metrics.uptime
                },
                discovery_method="integrated_test",
                tags={'discovered', 'integration_test'}
            )
        
        print(f"‚úÖ Cached {len(discovered_nodes)} discovered nodes")
        
        # Test cache retrieval
        print("üîç Testing cache retrieval...")
        cached_discovered = self.cache.get_nodes_by_tag('integration_test')
        print(f"‚úÖ Retrieved {len(cached_discovered)} nodes from cache")
        
        return len(discovered_nodes)
    
    async def test_cache_export_import(self):
        """Test cache export/import functionality"""
        print("\nüîÑ Testing Cache Export/Import...")
        print("=" * 50)
        
        # Export cache data
        print("üì§ Exporting cache data...")
        exported_data = self.cache.export_cache_data('json')
        print(f"  Exported data size: {len(exported_data)} characters")
        
        # Clear cache
        original_size = len(self.cache.cache)
        self.cache.cache.clear()
        print(f"  Cache cleared (was {original_size} nodes)")
        
        # Import cache data
        print("üì• Importing cache data...")
        self.cache.import_cache_data(exported_data, 'json')
        imported_size = len(self.cache.cache)
        print(f"  Imported {imported_size} nodes")
        
        if imported_size == original_size:
            print("‚úÖ Export/import test passed!")
        else:
            print("‚ùå Export/import test failed!")
        
        return imported_size == original_size
    
    async def run_all_tests(self):
        """Run all cache tests"""
        print("üöÄ STARTING NODE DISCOVERY CACHE TESTS")
        print("=" * 60)
        
        try:
            # Run individual tests
            cached_count = await self.test_basic_caching()
            await self.test_metrics_update()
            await self.test_location_and_tag_queries()
            persistence_passed = await self.test_cache_persistence()
            await self.test_cache_expiration()
            discovered_count = await self.test_integrated_discovery()
            export_import_passed = await self.test_cache_export_import()
            
            # Final statistics
            print("\nüìä FINAL CACHE STATISTICS")
            print("=" * 60)
            
            stats = self.cache.get_cache_stats()
            print(f"Cache Size: {stats['cache_size']}")
            print(f"Max Cache Size: {stats['max_cache_size']}")
            print(f"Active Nodes: {stats['active_nodes']}")
            print(f"Failed Nodes: {stats['failed_nodes']}")
            print(f"Hit Rate: {stats['hit_rate']:.1f}%")
            print(f"Average Latency: {stats['avg_latency']:.1f}ms")
            print(f"Average Reliability: {stats['avg_reliability']:.3f}")
            
            print(f"\nIndex Sizes:")
            print(f"  Host:Port Index: {stats['index_sizes']['host_port']}")
            print(f"  Location Index: {stats['index_sizes']['location']}")
            print(f"  Tag Index: {stats['index_sizes']['tag']}")
            
            print(f"\nCache Operations:")
            for key, value in stats['stats'].items():
                print(f"  {key}: {value}")
            
            # Test summary
            print("\nüéØ TEST SUMMARY")
            print("=" * 60)
            print(f"‚úÖ Basic caching: {cached_count} nodes cached")
            print(f"‚úÖ Metrics update: Working")
            print(f"‚úÖ Location/Tag queries: Working")
            print(f"{'‚úÖ' if persistence_passed else '‚ùå'} Cache persistence: {'Passed' if persistence_passed else 'Failed'}")
            print(f"‚úÖ Cache expiration: Working")
            print(f"‚úÖ Integrated discovery: {discovered_count} nodes discovered")
            print(f"{'‚úÖ' if export_import_passed else '‚ùå'} Export/Import: {'Passed' if export_import_passed else 'Failed'}")
            
            print("\nüéâ ALL TESTS COMPLETED!")
            
        except Exception as e:
            print(f"\n‚ùå TEST FAILED: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            # Cleanup
            print("\nüßπ Cleaning up...")
            self.cache.shutdown()
            print("‚úÖ Cache shutdown complete")

async def main():
    """Main test function"""
    test_suite = NodeDiscoveryCacheTest()
    await test_suite.run_all_tests()

if __name__ == "__main__":
    asyncio.run(main())
